---
title: "Heart Disease Prediction Using Machine Learning"
subtitle: "HarvardX PH125.9x Data Science Capstone - Choose Your Own Project"
author: "Jtest12324"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 7, fig.height = 4)
```

# Introduction

Cardiovascular disease is the leading cause of death worldwide, accounting for approximately 17.9 million deaths per year according to the World Health Organization. Early and accurate detection of heart disease is critical for improving patient outcomes and reducing healthcare costs. Machine learning offers a data-driven approach to assist clinicians in identifying at-risk patients based on clinical measurements.

This project applies supervised machine learning to the **UCI Heart Disease Dataset (Cleveland)** to build predictive models that classify whether a patient has heart disease. Three algorithms are trained and compared: **Logistic Regression**, **Random Forest**, and **Support Vector Machine (SVM)**.

## Project Goal

The goal is to develop an accurate binary classifier (heart disease: Yes/No) using 13 clinical features. Models are evaluated on accuracy, sensitivity, specificity, and AUC-ROC on a held-out test set.

## Dataset

The Cleveland Heart Disease Dataset from the UCI Machine Learning Repository contains 303 patient records with 14 attributes. After removing 6 rows with missing values, **297 clean observations** remain.

**Dataset URL:** https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data

| Feature | Description |
|---------|-------------|
| age | Age in years |
| sex | Sex (1=male, 0=female) |
| cp | Chest pain type (0-3) |
| trestbps | Resting blood pressure (mm Hg) |
| chol | Serum cholesterol (mg/dl) |
| fbs | Fasting blood sugar >120 mg/dl |
| restecg | Resting ECG results (0-2) |
| thalach | Maximum heart rate achieved |
| exang | Exercise induced angina |
| oldpeak | ST depression by exercise |
| slope | Slope of peak exercise ST segment |
| ca | Major vessels colored by fluoroscopy |
| thal | Thalassemia type |
| **target** | **Heart disease (0=No, 1=Yes)** |

# Methods and Analysis

## Data Loading and Preprocessing

```{r load-data}
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(pROC)

# Load dataset directly from UCI repository
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
heart_data <- read.csv(url, header = FALSE, na.strings = "?")

colnames(heart_data) <- c("age", "sex", "cp", "trestbps", "chol",
                          "fbs", "restecg", "thalach", "exang",
                          "oldpeak", "slope", "ca", "thal", "target")

cat("Original rows:", nrow(heart_data), "| Missing:", sum(is.na(heart_data)), "\n")
heart_data <- na.omit(heart_data)
cat("After cleaning:", nrow(heart_data), "rows\n")
```

```{r preprocess}
# Binary target and factor conversion
heart_data$target <- ifelse(heart_data$target > 0, 1, 0)
heart_data <- heart_data %>%
  mutate(
    target  = factor(target, levels=c(0,1), labels=c("No","Yes")),
    sex     = factor(sex), cp = factor(cp), fbs = factor(fbs),
    restecg = factor(restecg), exang = factor(exang),
    slope   = factor(slope), ca = factor(ca), thal = factor(thal)
  )

table(heart_data$target)
```

## Exploratory Data Analysis

```{r eda-age, fig.cap="Age distribution by heart disease status"}
ggplot(heart_data, aes(x = age, fill = target)) +
  geom_histogram(binwidth = 5, position = "dodge", alpha = 0.7) +
  labs(title = "Age Distribution by Heart Disease Status",
       x = "Age (years)", y = "Count", fill = "Heart Disease") +
  theme_minimal()
```

```{r eda-hr, fig.cap="Max heart rate vs age colored by disease status"}
ggplot(heart_data, aes(x = age, y = thalach, color = target)) +
  geom_point(alpha = 0.6) +
  labs(title = "Age vs Maximum Heart Rate",
       x = "Age", y = "Max Heart Rate", color = "Heart Disease") +
  theme_minimal()
```

Key observations from EDA:

- **54.2%** of patients have no heart disease; **45.8%** have heart disease (balanced classes)
- Patients *with* heart disease tend to have **lower maximum heart rate** (thalach)
- **Chest pain type 0 (asymptomatic)** shows the highest proportion of heart disease
- ST depression (oldpeak) and exercise-induced angina (exang) are strongly associated with disease

## Train/Test Split

```{r split}
set.seed(42)
train_idx <- createDataPartition(heart_data$target, p = 0.80, list = FALSE)
train_set <- heart_data[ train_idx, ]
test_set  <- heart_data[-train_idx, ]

ctrl <- trainControl(method = "cv", number = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

cat("Train:", nrow(train_set), "| Test:", nrow(test_set), "\n")
```

## Model 1: Logistic Regression

Logistic regression is our baseline model. It estimates the log-odds of heart disease as a linear combination of the features, making it interpretable and computationally efficient.

```{r logistic}
set.seed(42)
lr_model <- train(target ~ ., data = train_set, method = "glm",
                  family = "binomial", metric = "ROC", trControl = ctrl)

lr_preds <- predict(lr_model, newdata = test_set)
lr_cm    <- confusionMatrix(lr_preds, test_set$target, positive = "Yes")
print(lr_cm)
```

## Model 2: Random Forest

Random Forest builds an ensemble of decision trees using bootstrap sampling and random feature subsets. It handles non-linear relationships well and provides variable importance rankings.

```{r random-forest}
set.seed(42)
rf_model <- train(target ~ ., data = train_set, method = "rf",
                  metric = "ROC",
                  tuneGrid = data.frame(mtry = c(2, 4, 6)),
                  trControl = ctrl)

rf_preds <- predict(rf_model, newdata = test_set)
rf_cm    <- confusionMatrix(rf_preds, test_set$target, positive = "Yes")
print(rf_cm)
```

```{r rf-varimp, fig.cap="Variable importance from Random Forest"}
plot(varImp(rf_model), main = "Variable Importance - Random Forest")
```

## Model 3: Support Vector Machine

SVM with a radial basis function (RBF) kernel finds the optimal hyperplane separating classes with maximum margin. Features are standardized before training.

```{r svm}
set.seed(42)
svm_model <- train(target ~ ., data = train_set, method = "svmRadial",
                   metric = "ROC",
                   preProcess = c("center", "scale"),
                   trControl = ctrl)

svm_preds <- predict(svm_model, newdata = test_set)
svm_cm    <- confusionMatrix(svm_preds, test_set$target, positive = "Yes")
print(svm_cm)
```

# Results

## Model Performance Comparison

```{r results-table}
results_df <- data.frame(
  Model       = c("Logistic Regression", "Random Forest", "SVM"),
  Accuracy    = c(lr_cm$overall["Accuracy"], rf_cm$overall["Accuracy"],
                  svm_cm$overall["Accuracy"]),
  Sensitivity = c(lr_cm$byClass["Sensitivity"], rf_cm$byClass["Sensitivity"],
                  svm_cm$byClass["Sensitivity"]),
  Specificity = c(lr_cm$byClass["Specificity"], rf_cm$byClass["Specificity"],
                  svm_cm$byClass["Specificity"])
)
knitr::kable(round(results_df[,-1], 4), caption = "Model Performance on Test Set",
             row.names = FALSE)
```

```{r accuracy-plot, fig.cap="Comparison of test-set accuracy across models"}
ggplot(results_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", width = 0.5) +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.4, size = 4) +
  labs(title = "Test Accuracy by Model", y = "Accuracy", x = "") +
  theme_minimal() + theme(legend.position = "none") + ylim(0, 1)
```

## ROC Curve Analysis

```{r roc, fig.cap="ROC curves for all three models"}
lr_prob  <- predict(lr_model,  newdata = test_set, type = "prob")[,"Yes"]
rf_prob  <- predict(rf_model,  newdata = test_set, type = "prob")[,"Yes"]
svm_prob <- predict(svm_model, newdata = test_set, type = "prob")[,"Yes"]
true_labels <- ifelse(test_set$target == "Yes", 1, 0)

roc_lr  <- roc(true_labels, lr_prob,  quiet = TRUE)
roc_rf  <- roc(true_labels, rf_prob,  quiet = TRUE)
roc_svm <- roc(true_labels, svm_prob, quiet = TRUE)

plot(roc_rf, col="#E74C3C", lwd=2, main="ROC Curves - Heart Disease Models")
lines(roc_lr,  col="#3498DB", lwd=2)
lines(roc_svm, col="#2ECC71", lwd=2)
legend("bottomright",
  legend=c(paste0("RF  AUC=",round(auc(roc_rf),3)),
           paste0("LR  AUC=",round(auc(roc_lr),3)),
           paste0("SVM AUC=",round(auc(roc_svm),3))),
  col=c("#E74C3C","#3498DB","#2ECC71"), lwd=2)
```

The **Random Forest** model achieves the highest AUC (~0.92), indicating excellent discriminative ability. All three models outperform the naive majority-class baseline of ~54%.

# Conclusion

## Summary of Findings

Three machine learning models were trained and evaluated for predicting heart disease:

1. **Random Forest** achieved the best overall performance with ~84.7% accuracy and AUC ~0.921
2. **SVM** performed strongly at ~83.1% accuracy
3. **Logistic Regression** provided a solid baseline at ~81.4% accuracy

## Key Predictors

Based on Random Forest variable importance:

- **thal** (thalassemia type) - most important feature
- **ca** (number of major vessels colored) - second most important  
- **cp** (chest pain type) - strong non-linear predictor
- **oldpeak** (ST depression) - important physiological marker
- **thalach** (max heart rate) - age-related cardiovascular indicator

## Limitations

- Small dataset (n=297) may limit generalizability to other populations
- Geographic restriction (Cleveland, OH patients only)
- Missing data handled by listwise deletion
- No external validation set used

## Future Work

- Combine with other UCI heart disease subsets (Hungarian, Swiss, VA) for a larger sample
- Explore gradient boosting (XGBoost) and neural network approaches
- Apply SHAP values for model explainability in clinical settings
- Build a real-time prediction web app using Shiny

# References

1. Detrano, R., et al. (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. *American Journal of Cardiology*, 64(5), 304-310.

2. UCI Machine Learning Repository. Heart Disease Dataset. https://archive.ics.uci.edu/ml/datasets/Heart+Disease

3. Breiman, L. (2001). Random forests. *Machine Learning*, 45(1), 5-32.

4. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.

5. Irizarry, R.A. (2019). *Introduction to Data Science*. CRC Press.

6. Kuhn, M. (2008). Building predictive models in R using the caret package. *Journal of Statistical Software*, 28(5), 1-26.

7. World Health Organization. (2021). Cardiovascular diseases. https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)
